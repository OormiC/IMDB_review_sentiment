{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRPRyJCj7xKS41ysbvgcVW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OormiC/IMDB_review_sentiment/blob/main/Project1IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --q --upgrade keras-nlp\n",
        "!pip install --q --upgrade keras"
      ],
      "metadata": {
        "id": "59Rx10PFYoPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "\n",
        "# Tools for data manipulation\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Tools for building model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras_nlp\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Tools for preprocessing data\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import gensim\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Read in parquet files as pandas dataframes\n",
        "df_train = pd.read_parquet('/train-00000-of-00001.parquet', engine='pyarrow')\n",
        "df_test = pd.read_parquet('/test-00000-of-00001.parquet', engine='pyarrow')\n",
        "df = pd.concat([df_train, df_test])\n",
        "print(len(df))\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR3Ji2OaKVwM",
        "outputId": "7308cb0a-b078-4539-f406-b30876291a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n",
            "                                                text  label\n",
            "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
            "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
            "2  If only to avoid making this type of film in t...      0\n",
            "3  This film was probably inspired by Godard's Ma...      0\n",
            "4  Oh, brother...after hearing about this ridicul...      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function variables\n",
        "wnl = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def _clean(data):\n",
        "    # Filter out the non-meaningful words\n",
        "    meaning = [x for x in data.split() if x not in stop_words]\n",
        "    review_text = \" \".join(meaning)\n",
        "    # Remove HTML\n",
        "    review_text = BeautifulSoup(review_text, \"lxml\").get_text()\n",
        "    # Remove non-letters\n",
        "    letters_only = re.compile(r'[^A-Za-z\\s]').sub(\"\", review_text)\n",
        "    # Convert to lower case\n",
        "    data = letters_only.lower()\n",
        "\n",
        "    return data\n",
        "\n",
        "def _lemmatize(tokens: list) -> list:\n",
        "    # 1. Lemmatize\n",
        "    lemmatized_tokens = [wnl.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "def _preprocess(review):\n",
        "    # 1. Clean text\n",
        "    review = _clean(review)\n",
        "\n",
        "    # 2. Tokenize\n",
        "    tokens = word_tokenize(review)\n",
        "\n",
        "    # 3. Lemmatize\n",
        "    lemma = _lemmatize(tokens)\n",
        "\n",
        "    # return the result.\n",
        "    return lemma\n",
        "\n",
        "data_to_list = df['text'].values.tolist()\n",
        "review_list = [_preprocess(review) for review in data_to_list]\n",
        "\n",
        "print(review_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrJ81Yh20oxw",
        "outputId": "c83bb029-aa27-464c-97a1-74d98949ae9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-8f1c7c80b1b6>:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  review_text = BeautifulSoup(review_text, \"lxml\").get_text()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'rented', 'i', 'am', 'curiousyellow', 'video', 'store', 'controversy', 'surrounded', 'first', 'released', 'i', 'also', 'heard', 'first', 'seized', 'u', 'custom', 'ever', 'tried', 'enter', 'country', 'therefore', 'fan', 'film', 'considered', 'controversial', 'i', 'really', 'see', 'myselfthe', 'plot', 'centered', 'around', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'want', 'learn', 'everything', 'life', 'in', 'particular', 'want', 'focus', 'attention', 'making', 'sort', 'documentary', 'average', 'swede', 'thought', 'certain', 'political', 'issue', 'vietnam', 'war', 'race', 'issue', 'united', 'state', 'in', 'asking', 'politician', 'ordinary', 'denizen', 'stockholm', 'opinion', 'politics', 'sex', 'drama', 'teacher', 'classmate', 'married', 'menwhat', 'kill', 'i', 'am', 'curiousyellow', 'year', 'ago', 'considered', 'pornographic', 'really', 'sex', 'nudity', 'scene', 'far', 'between', 'even', 'shot', 'like', 'cheaply', 'made', 'porno', 'while', 'countryman', 'mind', 'find', 'shocking', 'reality', 'sex', 'nudity', 'major', 'staple', 'swedish', 'cinema', 'even', 'ingmar', 'bergman', 'arguably', 'answer', 'good', 'old', 'boy', 'john', 'ford', 'sex', 'scene', 'filmsi', 'commend', 'filmmaker', 'fact', 'sex', 'shown', 'film', 'shown', 'artistic', 'purpose', 'rather', 'shock', 'people', 'make', 'money', 'shown', 'pornographic', 'theater', 'america', 'i', 'am', 'curiousyellow', 'good', 'film', 'anyone', 'wanting', 'study', 'meat', 'potato', 'no', 'pun', 'intended', 'swedish', 'cinema', 'but', 'really', 'film', 'much', 'plot']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform our array of text into 2D numeric arrays\n",
        "max_words = 5000\n",
        "max_len = 200\n",
        "\n",
        "# Train set\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(review_list)\n",
        "sequences = tokenizer.texts_to_sequences(review_list)\n",
        "padded_list = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Examples of padded review\n",
        "print(padded_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOAz1oLuZvC7",
        "outputId": "176e3b6a-1049-453a-a51e-b5e43389ffb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    1 1424    1 2251  298  921 3305   30\n",
            "  549    1   27  480   30   91   57  678 2212  502 1445  124    3 1118\n",
            " 2685    1   18   17   48  112  113 3383  368  711  694   68  751  201\n",
            "   37   44  756   68  803  601  182  312  507  785  109  681  912  742\n",
            " 2317  199 1163  742 2073  632   44 1980 3627 1737  529 2382  314  368\n",
            " 1270  979  301    1 2251   56  546 1118   18  314  986   19  151 3384\n",
            "   13  150    7   33 3575  367  253   65 1472  524  314  986  565 3383\n",
            "  362   13 2848 4298  994   10   94  230  237 1519  314   19  561  101\n",
            "  314  516    3  516 1512 1018  168 1262   23   16  220  516  538  750\n",
            "    1 2251   10    3  174 1549 1567 3114  224 3434 1263 3383  362   40\n",
            "   18    3   21   48]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define train and test data\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_list, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Build, compile, and fit an RNN (single LSTM) model\n",
        "model1 = Sequential()\n",
        "model1.add(layers.Embedding(max_words, 20)) #The embedding layer\n",
        "model1.add(layers.LSTM(15,dropout=0.5)) #Our LSTM layer\n",
        "model1.add(layers.Dense(2,activation='sigmoid')) # Two possible outcomes (negative or positive)\n",
        "\n",
        "model1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint1 = ModelCheckpoint(\"best_model1.keras\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', save_weights_only=False)\n",
        "history = model1.fit(X_train, y_train, epochs=5,validation_data=(X_test, y_test),callbacks=[checkpoint1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m1DW2fw2WPp",
        "outputId": "7067ab01-0c27-4c94-a281-2c2d8f60eb5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7464 - loss: 0.4882\n",
            "Epoch 1: val_accuracy improved from -inf to 0.87240, saving model to best_model1.keras\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 71ms/step - accuracy: 0.7465 - loss: 0.4881 - val_accuracy: 0.8724 - val_loss: 0.3248\n",
            "Epoch 2/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8852 - loss: 0.2884\n",
            "Epoch 2: val_accuracy did not improve from 0.87240\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 65ms/step - accuracy: 0.8852 - loss: 0.2884 - val_accuracy: 0.8656 - val_loss: 0.3789\n",
            "Epoch 3/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8959 - loss: 0.2658\n",
            "Epoch 3: val_accuracy improved from 0.87240 to 0.88640, saving model to best_model1.keras\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 65ms/step - accuracy: 0.8959 - loss: 0.2658 - val_accuracy: 0.8864 - val_loss: 0.2837\n",
            "Epoch 4/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8997 - loss: 0.2534\n",
            "Epoch 4: val_accuracy did not improve from 0.88640\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 65ms/step - accuracy: 0.8997 - loss: 0.2534 - val_accuracy: 0.8851 - val_loss: 0.2892\n",
            "Epoch 5/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9021 - loss: 0.2465\n",
            "Epoch 5: val_accuracy did not improve from 0.88640\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 68ms/step - accuracy: 0.9021 - loss: 0.2465 - val_accuracy: 0.8854 - val_loss: 0.2842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display model loss and accuracy\n",
        "model_loss, model_accuracy = model1.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RusOXLGknWwU",
        "outputId": "bffe37c9-e585-4acd-c060-08005c7e209c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 6s - 19ms/step - accuracy: 0.8854 - loss: 0.2842\n",
            "Loss: 0.28420761227607727, Accuracy: 0.8853999972343445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Put in a review to test prediction\n",
        "sentiment = ['Negative','Positive']\n",
        "sequence = tokenizer.texts_to_sequences(['this movie is the best ever!'])\n",
        "test = pad_sequences(sequence, maxlen=max_len)\n",
        "sentiment[np.around(model1.predict(test), decimals=0).argmax(axis=1)[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xULThsI2_bfw",
        "outputId": "8f7914c1-695f-4c27-d685-7b44c1b99bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Positive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}